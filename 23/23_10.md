# 2023-10
## Paper List
1. From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels
2. Efficient One Pass Self-distillation with Zipf's Label Smoothing
3. Linearity Grafting: Relaxed Neuron Pruning Helps Certifiable Robustness
4. On the Integration of Self-Attention and Convolution
5. Stand-Alone Self-Attention in Vision Models
6. Local Relation Networks for Image Recognition
7. Syntax-Aware Network for Handwritten Mathematical Expression Recognition
8. 




## 10.15
### From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels
>http://arxiv.org/abs/2303.13005

![img](/img/23-10-1.png)

```
DKD的自蒸馏版
- 提出一种对目标、非目标类分别生成伪标签的方法，提高蒸馏效率
- 传统师生non-target logits总和不同，妨碍对齐与学习, 引入标准化KD(NKD)
- 训练成本小
- sota

整体框架:
对于tareget label,由于学生开始预测变化很大,修炼中平滑目标输出
对non-target label,采用Zipf方法重新排序分布

对比related创新点:
1. DKD需要调参,不够高效
2. 对比Zipf, 能够对于Vit框架使用排列

实验对比:
对比的方法较少, 感觉效果不如mixkd这种训练成本高的蒸馏效果好
```

---
### Efficient One Pass Self-distillation with Zipf's Label Smoothing
>http://arxiv.org/abs/2207.12980

![img](/img/23-10-2.png)
```
齐夫定律是哈佛大学语言学家乔治·齐夫（George Zipf）1949年发现的一个实验定律，即在自然语言里，一个单词出现的频率与它在频率表里的排序成反比。
```
```
通过汇聚层之前的卷积特征(中间特征)计算空间上的argmax, 作为依据生成non-target label辅助模型自蒸馏

创新:
- 通过中间特征引导生成label,可解释性很好,改进空间很大
- 简单,即插即用

将图像的空间属性和概率挂钩是否合理?小目标,密集目标
和注意力结合来改进?
对没出现的类别给予恒定的分布值是否合理?
```

## 10.16-17
### Linearity Grafting: Relaxed Neuron Pruning Helps Certifiable Robustness
>http://arxiv.org/abs/2206.07839

![img](/img/23-10-3.png)

```
可验证鲁棒
对relu线性化，等效替代成wx+b的形式，对不重要的激活层用线性函数代替
灵感来源于剪枝策略，剪枝等价于斜率为0的线性函数，用来减少验证的非线性

relate：
pruning and robustness
```

### On the Integration of Self-Attention and Convolution
>http://arxiv.org/abs/2111.14556

![img](/img/23-10-4.png)
```
整合了cnn和self-attention操作一体化模型，通过超参数合并输出，计算成本小
整体框架：
- 一阶段共用卷积特征，二阶段的额外计算开销小
- 卷积特征对卷积核等价拆解为1*1卷积通过移位操作合并
- 自注意力通过--Stand-Alone Self-Attention in Vision Models提出的自注意力方法计算
 
消融实验：
- 自注意力起到更大的作用，且直观来看加上卷积后提升不大，并且组合的方式影响不大
- 早期卷积带来更大提升，后期自注意力作用更大
```

## 10.18
### Stand-Alone Self-Attention in Vision Models
>http://arxiv.org/abs/1906.05909

![img](/img/23-10-5.png)
```
self attention
出发点：对于卷积模型无法作用于长距离的特征建模，在大感受野方面的缩放特性较差

相关工作：non_local,'Attention Augmented Convolutional Networks','Local Relation Networks'

全局的注意力需要极大的计算量，本文仅在局部空间计算attention，取代传统CNN
通过2d卷积分别计算出qkv，经过softmax后和value计算加权和逐像素得到输出结果（类似卷积），下采样通过2*2池化进行。
编码： 相对编码>绝对编码>无编码
```

## 10.24
### Local Relation Networks for Image Recognition
>http://arxiv.org/abs/1904.11491

![img](/img/23-10-6.png)
```
卷积的一种替代
- 对大卷积核计算更加高效
- 对抗攻击更加稳健，受益于空间域的相关性组合
```

## 10.27
### Syntax-Aware Network for Handwritten Mathematical Expression Recognition
>http://arxiv.org/abs/2203.01601

![img](/img/23-10-7.png)
```
经典树解码的公式识别论文，再DWAP-TD树解码基础上包含两点改进：
1. 优化了树解码的语法约束（主要贡献）
    - 论文申明公式识别的主要难点在于语法关系而不是符号识别(质疑，badcase很多字符混淆)，本文拆解公式为不同组件减轻难度
2. 优化注意力方式
    - 基于树的遍历路线计算覆盖注意力而不是用传统方法计算所有的，避免注意力漂移
    - 反向解码器预测父节点，本质是注意力的自纠正，推理阶段忽略

改进方向：
- 优化树解码
    - 简化相邻符号个数，例如：上、右上，左上（较少出现）合并为一种
    - 优化相对关系，图神经网络
- 强化视觉特征
- 基于树的数据增强
```
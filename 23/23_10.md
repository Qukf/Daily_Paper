# 2023-10
## Paper List
1. From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels
2. Efficient One Pass Self-distillation with Zipf's Label Smoothing
3. 
4. 





## 10.15
### From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels
>http://arxiv.org/abs/2303.13005

![img](/img/23-10-1.png)

```
DKD的自蒸馏版
- 提出一种对目标、非目标类分别生成伪标签的方法，提高蒸馏效率
- 传统师生non-target logits总和不同，妨碍对齐与学习, 引入标准化KD(NKD)
- 训练成本小
- sota

整体框架:
对于tareget label,由于学生开始预测变化很大,修炼中平滑目标输出
对non-target label,采用Zipf方法重新排序分布

对比related创新点:
1. DKD需要调参,不够高效
2. 对比Zipf, 能够对于Vit框架使用排列

实验对比:
对比的方法较少, 感觉效果不如mixkd这种训练成本高的蒸馏效果好
```

---
### Efficient One Pass Self-distillation with Zipf's Label Smoothing
>http://arxiv.org/abs/2207.12980

![img](/img/23-10-2.png)
```
齐夫定律是哈佛大学语言学家乔治·齐夫（George Zipf）1949年发现的一个实验定律，即在自然语言里，一个单词出现的频率与它在频率表里的排序成反比。
```
```
通过汇聚层之前的卷积特征(中间特征)计算空间上的argmax, 作为依据生成non-target label辅助模型自蒸馏

创新:
- 通过中间特征引导生成label,可解释性很好,改进空间很大
- 简单,即插即用

将图像的空间属性和概率挂钩是否合理?小目标,密集目标
和注意力结合来改进?
对没出现的类别给予恒定的分布值是否合理?
```